{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we load the dataset an perform the necessary preprocessing steps to make it availavle for the analysis. In particular we go throught the following steps:\n",
    "\n",
    "- Load all the .arff files in pandas dataframes\n",
    "- convert all the numeric values of the dataframes from strings to floats\n",
    "- check that the only string in the dfs (apart from the ones indicating the country) is 'm', which indicates a missing values\n",
    "- replace all missing values 'm' with the mean value of the corresponding column\n",
    "\n",
    "In particular, for the last step we first consider just the values that have the same country of the missing value. Then we calculate the mean. If there are no values about that country, we simply consider the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(folder='data'):\n",
    "    \"\"\"\n",
    "        Create DataFrame from ARFF files.\n",
    "\n",
    "        Returns:\n",
    "        - df_ls (list of pandas.DataFrame): List of DataFrames created from ARFF files.\n",
    "    \"\"\"\n",
    "\n",
    "    files = sorted(os.listdir(folder)) # Sorting files in the directory\n",
    "    df_ls = [] # Initializing list to store DataFrames\n",
    "\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        file = folder + \"/\" + f # Creating file path\n",
    "        dataset = arff.load(open(file, 'r')) # Loading ARFF file\n",
    "        df = pd.DataFrame(dataset['data']) # Creating DataFrame\n",
    "        df.insert(0, 'Quarter', f[:7]) # Inserting 'Quarter' column\n",
    "        df_ls.append(df) # Appending DataFrame to the list\n",
    "\n",
    "    \n",
    "    return df_ls\n",
    "\n",
    "def convert_to_float(val):\n",
    "    \"\"\"\n",
    "        Convert a value to float.\n",
    "\n",
    "        Parameters:\n",
    "        - val: Value to be converted.\n",
    "\n",
    "        Returns:\n",
    "        - float_val: Converted float value.\n",
    "        \"\"\"\n",
    "    try:\n",
    "        return float(val) # Convert the value to float\n",
    "    except ValueError:\n",
    "        return val  # Return the original value for non-convertible strings\n",
    "\n",
    "def check_unique_strings(df_ls):\n",
    "    \"\"\"\n",
    "        Check if DataFrame contains only one unique string.\n",
    "\n",
    "        Parameters:\n",
    "        - df_ls (list of pandas.DataFrame): List of DataFrames.\n",
    "\n",
    "        Raises:\n",
    "        - ValueError: If DataFrame contains multiple unique strings.\n",
    "    \"\"\"\n",
    "    for i in range(len(df_ls)):\n",
    "        df = df_ls[i].iloc[:, 2:] # Extracting relevant columns from DataFrame\n",
    "\n",
    "        unique_strings = df.stack().unique() # Finding unique values in the DataFrame\n",
    "        # Filter out non-string values\n",
    "        strings = [string for string in unique_strings if isinstance(string, str)]\n",
    "                            \n",
    "        if len(strings) > 1: # Checking if more than one unique string exists\n",
    "            raise ValueError(\"The DataFrame contains multiple unique strings, 'm' is not the only one\")\n",
    "\n",
    "\n",
    "def check_specific_value(df_ls, specific_value='m'):\n",
    "    \"\"\"\n",
    "        Check if a specific value exists as the only unique value in a DataFrame column.\n",
    "\n",
    "        Parameters:\n",
    "        - df_ls (list of pandas.DataFrame): List of DataFrames.\n",
    "        - specific_value (str): Specific value to be checked. Default is 'm'.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(df_ls)):\n",
    "        df = df_ls[i]\n",
    "        cols = df.columns[2:-1]\n",
    "\n",
    "        for col in cols:\n",
    "            # Count the occurrences of each unique value in the column\n",
    "            value_counts = df[col].value_counts()\n",
    "\n",
    "            # Check if there is only one unique value and if it matches the specific value\n",
    "            if len(value_counts) == 1 and value_counts.index[0] == specific_value:\n",
    "                print(f\"In DataFrame {i}, the column '{col}' contains only the specific value '{specific_value}'.\")\n",
    "\n",
    "\n",
    "def replace_m(df_ls):\n",
    "    \"\"\"\n",
    "        Replace 'm' values in DataFrame with the mean of non-'m' values.\n",
    "\n",
    "        Parameters:\n",
    "        - df_ls (list of pandas.DataFrame): List of DataFrames.\n",
    "\n",
    "        Returns:\n",
    "        - df_ls (list of pandas.DataFrame): List of DataFrames with 'm' replaced.\n",
    "    \"\"\"\n",
    "    for i in range(len(df_ls)):\n",
    "        df = df_ls[i]\n",
    "        countries = np.unique(df[0].values) # Get unique country values\n",
    "        cols = df.columns[2:-1] # Get relevant columns excluding the first two and the last one\n",
    "\n",
    "        for col in cols: # Iterate over columns\n",
    "            for country in countries:\n",
    "\n",
    "                mask1 = df[col] != 'm' # Create mask to filter out 'm' values\n",
    "                mask2 = df[0] == country # Create mask to filter out rows corresponding to the current country\n",
    "\n",
    "                mask_1_2_arr = df[mask1 & mask2][col].values # Apply both masks and get the values\n",
    "                mask_1_arr = df[mask1][col].values # Apply only the first mask and get the values\n",
    "\n",
    "                if len(mask_1_2_arr)!=0: # Check if there are non-'m' values for the current country\n",
    "                    df[col].replace('m', np.mean(mask_1_2_arr), inplace=True) # Replace 'm' with mean of non-'m' values\n",
    "                elif len(mask_1_2_arr)==0 and len(mask_1_arr)!=0: # Check if there are non-'m' values for other countries\n",
    "                    df[col].replace('m', np.mean(mask_1_arr), inplace=True) # Replace 'm' with mean of non-'m' values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ls = create_df(folder='data')\n",
    "\n",
    "for df in df_ls:\n",
    "    df.iloc[:, 2:] = df.iloc[:, 2:].applymap(convert_to_float) # Convert values to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unique_strings(df_ls)\n",
    "check_specific_value(df_ls, specific_value='m')\n",
    "replace_m(df_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis of Pre/Post Covid trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we analyze the varation of the features of the dataset before and after Covid (pre <= Q1 2020, after >= Q2 2020). To evaluate the variation we first split the dataset in pre and post covid. Then we have to verify if there is a statistically significant change. Since we have many features to analyze we need a metric able to summarize. A possible solution would be to calculate for each feature the mean value and observe the variation. But during the analysis we have observed that some features are characterized by large otuliers, which strongly influence the mean value and could lead to a misleading analysis. For this reason we choose a metric that still gives an estimate of the value of the feature and it is not affeted by outliers: the median. So what we finally calculate is:\n",
    "\n",
    "- variation = |(median_pre -  median_post)/median_pre| * 100\n",
    "\n",
    "In the case in which the median_pre is 0 we simply do variation = |median_pre -  median_post|*100 to avoid division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_variation(median_pre,median_post):\n",
    "    \"\"\"\n",
    "        Calculate the percentage variation between two mean values.\n",
    "\n",
    "        Parameters:\n",
    "        - mean_pre (float): Mean value before a certain event.\n",
    "        - mean_post (float): Mean value after a certain event.\n",
    "\n",
    "        Returns:\n",
    "        - variation (float): Percentage variation between mean_pre and mean_post.\n",
    "    \"\"\"\n",
    "\n",
    "    n = median_post - median_pre # Calculating the difference between\n",
    "    if median_pre==0.: # Checking if mean_pre is equal to 0\n",
    "        return np.abs(n)*100 # Calculating and returning absolute variation\n",
    "    else:\n",
    "        return np.abs(n/median_pre)*100 # Calculating and returning relative variation\n",
    "\n",
    "\n",
    "def plot_variations(variations,title='',custom_y_tick=False,color='skyblue', figsize=(15, 10), fontsize=25):\n",
    "    \"\"\"\n",
    "        Plot the variations.\n",
    "\n",
    "        Parameters:\n",
    "        - variations (list): List of variation values for each feature.\n",
    "        - title (str): Title for the plot. Default is an empty string.\n",
    "        - custom_y_tick (bool): Whether to use custom y-axis ticks. Default is False.\n",
    "        - color (str): Color for the bars in the plot. Default is 'skyblue'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the x labels and positions\n",
    "    x_labels = np.arange(len(variations))+1\n",
    "    x_positions = np.arange(len(variations))\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Plot the bar chart with adjusted width\n",
    "    plt.bar(x_positions, variations, width=0.6,color=color, edgecolor='black')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Feature', fontsize=fontsize)\n",
    "    plt.ylabel('% Variation', fontsize=fontsize)\n",
    "    plt.title(f'% Variation {title} before and after Covid (pre <= Q1 2020, after >= Q2 2020)', fontsize=fontsize+2, pad=20)\n",
    "\n",
    "    # Set custom x tick labels\n",
    "    plt.xticks(x_positions, x_labels, fontsize=10)\n",
    "\n",
    "    if custom_y_tick==True:\n",
    "        # Set y ticks \n",
    "        plt.yticks(np.arange(0, max(variations) + 25, 25), fontsize=16)\n",
    "\n",
    "\n",
    "    # Rotate x tick labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    name = f'% Variation {title} before and after Covid (pre <= Q1 2020, after >= Q2 2020)'\n",
    "    #plt.savefig(\"plots/\"+name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_ls = [np.unique(df['Quarter'].values) for df in df_ls] # Extracting unique quarters from each DataFrame\n",
    "idx = Q_ls.index('2020 Q2') # Finding the index of '2020 Q2'\n",
    "\n",
    "df_ls_pre = df_ls[:idx] # Slicing DataFrame list to get pre-COVID data\n",
    "df_ls_post = df_ls[idx:] # Slicing DataFrame list to get post-COVID data\n",
    "\n",
    "concatenated_df_pre = pd.concat(df_ls_pre) # Concatenating pre-COVID DataFrames\n",
    "concatenated_df_post = pd.concat(df_ls_post) # Concatenating post-COVID DataFrames\n",
    "\n",
    "variations = []\n",
    "\n",
    "for col in df_ls[0].columns[2:-1]: # Iterating over columns excluding first two and last one\n",
    "\n",
    "    vals_pre = concatenated_df_pre[col].values\n",
    "    vals_post = concatenated_df_post[col].values\n",
    "\n",
    "    median_pre = np.median(vals_pre) # Calculating median of pre-COVID values\n",
    "    median_post = np.median(vals_post) # Calculating median of post-COVID values\n",
    "\n",
    "    variation = calc_variation(median_pre,median_post)\n",
    "\n",
    "    variations.append(variation)\n",
    "\n",
    "plot_variations(variations,custom_y_tick=True) # Plotting variations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis suggest that the feature that change the most is the one of column 77 (X_77), since its value change more the 200%. But there are still variables that have a variation of 100% like X_6, X_10 and others. The are also variable with a less evident but still notable change like X_17 and X_35, which have a variation of more or less 70%\n",
    "\n",
    "Now we perform the same analysis but considering each sector indipendently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_high_variations(count_dict,variations, threshold=75):\n",
    "    \"\"\"\n",
    "    Count the number of times each feature (column) has a variation greater than or equal to a threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - variations (list): List of variations for each feature.\n",
    "    - threshold (float): The threshold for considering a variation as high. Default is 75.\n",
    "\n",
    "    Returns:\n",
    "    - count_dict (dict): Dictionary where keys are feature names and values are the counts of high variations.\n",
    "    \"\"\"\n",
    "\n",
    "    for idx, variation in enumerate(variations):\n",
    "        if variation >= threshold:\n",
    "            column_name = df_ls[0].columns[2:-1][idx] # Extract the column name corresponding to the index\n",
    "            count_dict[column_name] = count_dict.get(column_name, 0) + 1 # Increment the count for the column in the dictionary\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "count_dict = {}\n",
    "for sector in range(1,7):#Iterating over sectors and repeat the process\n",
    "    \n",
    "    variations = []\n",
    "    means_pre = []\n",
    "    means_post = []\n",
    "\n",
    "    concatenated_df_pre_masked = concatenated_df_pre[concatenated_df_pre[83] == sector]\n",
    "    concatenated_df_post_masked = concatenated_df_post[concatenated_df_post[83] == sector]\n",
    "\n",
    "    for col in df_ls[0].columns[2:-1]:\n",
    "\n",
    "        vals_pre = concatenated_df_pre_masked[col].values\n",
    "        vals_post = concatenated_df_post_masked[col].values\n",
    "\n",
    "        median_pre = np.median(vals_pre)\n",
    "        median_post = np.median(vals_post)\n",
    "\n",
    "        variation = calc_variation(median_pre,median_post)\n",
    "        variations.append(variation) \n",
    "\n",
    "    count_high_variations(count_dict,variations)\n",
    "    plot_variations(variations,custom_y_tick=True,title=\"(sector\"+str(sector)+\")\")\n",
    "\n",
    "sorted_count_dict = dict(sorted(count_dict.items(), key=lambda item: item[1], reverse=True)) #counts variations over the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to rank the financial indicator based on their change, we count how many times the have a change of over the 75%. As it is possible to see above, the features X_10,X_16,X_26,X_36,X_68 have a significant change in for all the sectors. While X_18,X_11,X_21,X_45,X_59 changes signficantly for 5 out of 6 secotrs. \n",
    "Talking about the ablsolute values of the variations, X_51 has a large variation in Sector 1 and in Sector 4. In Sector 1 also X_22 changes more than 100%. X_77 in Sector 2 and Secotr 3 is characterized by a big change. Finally, in Sector 4 X_29 and X_33 change more than 200%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector Classifier: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we train two Sector classifiers. In particular we consider XGBoost and RandomForest, since they are two kind of models particularly suitable for tabular data analysis. In particular, XGBoost has proven to achieve better performances but we also consider RandomForest for its better interpretability.\n",
    "\n",
    "We have to perform some preprocessing steps before the training. In particular we have to drop the columns that w/o the sector, because in those cases we would miss the label. Fortunately, they are only 8. Then we have to trasnform the  labels values from [1,2,3,4,5,6] to [0,1,2,3,4,5]. Finally we have to encode the variable indicating the country into a categoorical one.\n",
    "\n",
    "We obviously split the dataset into train and test, and we measure models accuracy on the test set.\n",
    "\n",
    "FOr the tuning of the Hyperparameters of the models, we have used a Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot = pd.concat(df_ls)\n",
    "\n",
    "rows_dropped = len(df_tot[df_tot[83] == 'm'])  #Count number of rows with 'm' values of column with labels\n",
    "df_tot.drop(df_tot[df_tot[83] == 'm'].index, inplace=True) # Drop rows with 'm' values of column with labels\n",
    "print(\"dropped rows w/o label:\",rows_dropped)\n",
    "\n",
    "df_tot[83] = df_tot[83]-1 # Decrement the values in column 83 by 1\n",
    "df_tot[83] = df_tot[83].astype(int) # Convert column 83 to integer type\n",
    "label_encoder = LabelEncoder() # Initialize label encoder\n",
    "df_tot[0] = label_encoder.fit_transform(df_tot[0]) # Encode values in column 0\n",
    "\n",
    "df = df_tot.iloc[:,1:] # Extract features for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_split_df(df, test_size=0.2, random_state=None):\n",
    "    # Separate features and labels\n",
    "    X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
    "    y = df.iloc[:, -1]   # Labels (last column)\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [200,300,400,500],\n",
    "    'max_depth': [10,15,20,50],\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "}\n",
    "\n",
    "# Create a XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', num_class=6, random_state=42)\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_grid,\n",
    "                                   n_iter=15, scoring='accuracy', cv=3, verbose=3, random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model with the best parameters on the entire training set\n",
    "final_model = xgb.XGBClassifier(objective='multi:softmax', num_class=6, random_state=42, **best_params)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Calculate recall, precision, and F1 score\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Accuracy of XGB:\", accuracy)\n",
    "print(\"Recall of XGB:\", recall)\n",
    "print(\"Precision of XGB:\", precision)\n",
    "print(\"F1 Score of XGB:\", f1)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6'], yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 4', 'Class 5', 'Class 6'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix of XGB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost reaches an accuracy of almost the 80%, which is a quite good result. Looking at the values of the precision and recall and at the confusion matrix, we notice that precision is higher than recall. It means that while it is good at identifying true positives, it may miss some of the actual positive instances present in the data. In other words, the model is conservative in its predictions, preferring to make fewer positive predictions overall but ensuring that those it does make are more likely to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 400, 500],\n",
    "    'max_depth': [10, 15, 20, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(class_weight='balanced',random_state=42)\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search_rf = RandomizedSearchCV(estimator=rf_clf, param_distributions=param_grid,\n",
    "                                      n_iter=15, scoring='accuracy', cv=3, verbose=3, random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV on the training data\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_rf = random_search_rf.best_params_\n",
    "print(\"Best Parameters for Random Forest:\", best_params_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest classifier with the best parameters\n",
    "final_model = RandomForestClassifier(**best_params_rf, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier on the training data\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Calculate recall, precision, and F1 score\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Accuracy of Random Forest:\", accuracy)\n",
    "print(\"Recall of Random Forest:\", recall)\n",
    "print(\"Precision of Random Forest:\", precision)\n",
    "print(\"F1 Score of Random Forest:\", f1)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6'], yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 4', 'Class 5', 'Class 6'])\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix of Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have that the accuracy is lower than the one of the XGBoost: 71%. We still observe an higher precision with respect to the recall, which leads to the same discussion we made for the XGBoost.\n",
    "\n",
    "From the analysis we have observed that XGBoost ia a better.\n",
    "\n",
    "Finally, even if the XGBoost is built to handle and class imbalance and that we made the Random Forest able to deal with it (class_weight='balanced'), it is clear that the fact that the majority of the dataset is populated by companies of the Sector 3 influences a lot the classification. Probably a more balanced dataset could lead to a performance improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_challenge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
